{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f79a70a1-842d-5acd-d706-01fc7584fc0b"
   },
   "source": [
    "# Final Project:  Spam filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "You’re the project manager for an enterprise email system and assigned a task to develop a spam filter for a company's email system. We’ve collected email samples that have been validated to be spam or non-spam emails. Your task is to predicts whether an email contains spam or not.\n",
    "\n",
    "You are given 3068 training emails with two classes: \"spam\" or \"not spam\". Using these data, you are expected to build your own spam filter with the kownledge you learned from this course. The goal is to correctly classify 1292 test emails. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "- **Spam email**\n",
    "\n",
    "> Subject: younger and healthier with ultimate - hghl 7283  as seen on nbc , cbs , cnn , and even oprah ! the health discovery that actuallyreverses aging while burning fat , without dieting or exercise ! this provendiscovery has even been reported on by the new england journal of medicine . forget aging and dieting forever ! and it ' s guaranteed !  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  would you like to lose weight while you sleep !  no dieting !  no hunger pains !  no cravings !  no strenuous exercise !  change your life forever !  100 % guaranteed !  1 . body fat loss 82 % improvement .  2 . wrinkle reduction 61 % improvement .  3 . energy level 84 % improvement .  4 . muscle strength 88 % improvement .  5 . sexual potency 75 % improvement .  6 . emotional stability 67 % improvement .  7 . memory 62 % improvement .  click below to enter our web site :  http : / / www . freehostchina . com / washgh /  if you want to get removed  from our list please email at - standardoptout @ x 263 . net ( subject = remove \" your email \" )\n",
    "\n",
    "- **Not-spam email**\n",
    "\n",
    "> Subject: december 6 th meeting  dear mr . kaminski :  this is to confirm the december 6 th meeting here at our center .  the location for the meeting is room # 3212 steinberg hall - dietrich hall and  the time will run from 9 : 00 am - 11 : 00 am .  please let us know if you need anything further .  we look forward to seeing you then .  regards ,  theresa convery  ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  theresa convery  administrative assistant  risk and decision processes center  the wharton school of the university of pennsylvania  ( 215 ) 898 - 5688 / fax : ( 215 ) 573 - 2130  tconvery @ wharton . upenn . edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of solution\n",
    "\n",
    "\n",
    "**1. Text representation.**\n",
    "\n",
    "As you can see, text content of emails is unstructured data. To apply machine learning methods on top of them, we first need to extract structured feature. To demonstrate this, we'll show using bag-of-word model for textural represention.\n",
    "\n",
    "** 2. Build your classifier. **\n",
    "\n",
    "As baseline, we provide solution based on *SVM* (Support Vector Machine).\n",
    "\n",
    "** 3. Evaluation **\n",
    "\n",
    "We will use *AP* (Average Precision) and *Accuracy* for performance evaluation in this notebook.\n",
    "\n",
    "Note that for evaluation on kaggle competition, [*MCE*](https://www.kaggle.com/wiki/MeanConsequentialError) (Mean Consequential Error) is used instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python package dependence\n",
    "- **pandas**   : for loading CSV files;\n",
    "- **nltk**     ：for word pre-processing;\n",
    "- **wordcloud**: for data visulization.\n",
    "\n",
    "Tips: To install missing packages, you can either do \"pip install package_name\" or \"conda install package_name\" in case of anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2c1ada8-d68d-4e8d-e807-6e47ea7f5a58"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "## 1) Download data.\n",
    "\n",
    "Download \"emails.train.csv\", \"emails.test.csv\" from our kaggle competition page [here](https://www.kaggle.com/c/spamfilter-aml-uva/data), and put it under the same folder as this ipython notebook.\n",
    "\n",
    "\n",
    "\n",
    "## 2) Read in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e14bfbc8-92b8-4b60-081d-1f6fc0728ab1",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import operator\n",
    "\n",
    "assert os.path.exists('./emails.train.csv'), \"[Dataset File Not Found] Please download dataset first.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "204b6823-c7ea-8f47-a62b-f649f82c71ee",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  spam\n",
      "0   0  Subject: naturally irresistible your corporate...     1\n",
      "1   2  Subject: unbelievable new homes made easy  im ...     1\n",
      "2   3  Subject: 4 color printing special  request add...     1\n",
      "3   4  Subject: do not have money , get software cds ...     1\n",
      "4   5  Subject: great nnews  hello , welcome to medzo...     1\n",
      "415\n",
      "0.23700571997\n",
      "0.23700571997\n"
     ]
    }
   ],
   "source": [
    "# Read in csv file as dataframe\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "zf = pd.read_csv('./emails.test.csv')\n",
    "# Show a snippet of dataset.\n",
    "print df.head()\n",
    "print np.sum(zf['spam'])\n",
    "spamappend = 0\n",
    "hamappend = 0\n",
    "for i in df[\"spam\"]:\n",
    "    if i == 1:\n",
    "        spamappend += 1\n",
    "    if i == 0:\n",
    "        hamappend +=1\n",
    "\n",
    "probspam = float(spamappend)/(spamappend+hamappend)\n",
    "print probspam\n",
    "spam = sum(df[\"spam\"])\n",
    "ham =  len(df[\"spam\"])-sum(df[\"spam\"])\n",
    "probspam = float(spam)/(spam+ham)\n",
    "print probspam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, each emails has two fields: \n",
    "* \"text\": the full text content of an email.\n",
    "* \"spam\": an integer flag to mark whether an email is a spam (=1) or not (=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Example of spam emails\n",
      "---------------------------\n",
      "Subject: looking for good it team ? we do software engineering !  looklng for a good lt team ?  there can be many reasons for hiring a professional  lt team . . .  - lf you ' ve qot an active on - line business and you  are dissatisfied with the guaiity of your currentsupport , its cost , or  both . . .  - lf your business is expanding and you ' re ionqing  for a professionai support team . . .  - lf you have specific software requirements and  you ' d iike to have your soiutions customized , toqetherwith warranties and  reiiabie support . . .  - if you have the perfect business idea and want to  make it a reality . . .  - if your project has stalled due to lack of  additional resources . . .  - if you need an independent team for benchmarking ,  optimization , quality assurance . . .  if you ' re looking for  a truly professional team , we are at your service ! just visit our  website  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_pos = df[df['spam']==1]\n",
    "\n",
    "print( np.random.choice(df_pos['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "Example of not spam emails\n",
      "---------------------------\n",
      "Subject: australian energy risk 2000  as australian energy risk 2000 is approaching . i am emailing you with  regards to your presentation . the deadline date is monday 12 june . if this  is going to be a problem please let me know i will be out of the office from  monday 12 , until monday 19 june , on which date i will be putting the  presentation pack together to send to the printers . if your presentation is  complete could you please email it to ldeathridge @ risk . co . uk as a  powerpoint file asap .  if you have any problems in doing this please do not hesitate to contact me  by email or on ( 020 ) 7484 9867 . i will reply to any queries when i return .  regards  lucie deathridge  conference co - ordinator  risk publications  ?  tel : ( + 44 ) ( 020 ) 7484 9867  http : / / www . riskpublications . com\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "print(\"===========================\")\n",
    "print(\"Example of not spam emails\")\n",
    "print(\"---------------------------\")\n",
    "\n",
    "df_neg = df[df['spam']==0]\n",
    "\n",
    "print( np.random.choice(df_neg['text']) ) \n",
    "\n",
    "print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation\n",
    "\n",
    "## 1) Create the bags of words vocabulary. \n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. (from [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word         frequency\n",
      "-------------------------\n",
      "-                72932\n",
      ".                65233\n",
      ",                42792\n",
      "the              34362\n",
      ":                29814\n",
      "to               28822\n",
      "/                27959\n",
      "and              19173\n",
      "of               16317\n",
      ">                14658\n",
      "a                13894\n",
      "you              13305\n",
      "in               12631\n",
      "i                12151\n",
      "for              11639\n",
      "...\n",
      "\n",
      "17\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# DIT ITEREERT DUS OVER DE PANDAS DATAFRAME\n",
    "def select_vocabulary(dataframe, topN=10000):\n",
    "    # for w in dataframe.str.lower():\n",
    "    word2freq = dict()\n",
    "    for line in dataframe:\n",
    "        words = line.split()\n",
    "#       met setdefault fixed ie de value in de dict op 0 tenzij er iets te tellen valt dan +1\n",
    "        for word in words:\n",
    "            word2freq[word] = word2freq.setdefault(word, 0) +1\n",
    "\n",
    "    word_freq = [ (word,freq) for word,freq in word2freq.items() ]\n",
    "    \n",
    "    # sort according to freq, in descending order.\n",
    "    word_freq.sort(key=lambda x:x[1], reverse=True) \n",
    "    \n",
    "    # show selection results\n",
    "    print(\"%-10s  %10s\" % ('word', 'frequency'))\n",
    "    print(\"-------------------------\")\n",
    "    for i in range(15):\n",
    "        print(\"%-10s  %10d\" % (word_freq[i]))\n",
    "    print(\"...\\n\")\n",
    "    \n",
    "#     returned top 100 meest voorkomende woorden\n",
    "    return([x[0] for x in word_freq[:topN]])\n",
    "\n",
    "vocabulary = select_vocabulary(df['text'])\n",
    "word2ind   = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "# dus als je print vocabulary[0] krijg je - en als je doet word2ind['-'] krijg je 0 gezien die het vaakst voorkomt\n",
    "print word2ind['enron']\n",
    "print vocabulary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Extract Bag-of-Word Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting feature for train ...\n",
      "   0 / 4021 \n",
      "1000 / 4021 \n",
      "2000 / 4021 \n",
      "3000 / 4021 \n",
      "4000 / 4021 \n",
      "4020 / 4021 \n",
      "Extracting feature for test ...\n",
      "   0 / 1707 \n",
      "1000 / 1707 \n",
      "1706 / 1707 \n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "def extract_Bag_of_Word_feature(dataframe):\n",
    "    BoWs = np.zeros((len(dataframe), len(vocabulary)), dtype=np.float32)\n",
    "\n",
    "    for i, line in enumerate(dataframe):\n",
    "        for word in line.split():\n",
    "            word_ind = word2ind.get(word, -1)\n",
    "            if(word_ind>=0):\n",
    "                BoWs[i, word_ind] += 1\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "    print(\"%4d / %d \" % (i, len(dataframe)))\n",
    "            \n",
    "    return BoWs\n",
    "\n",
    "\n",
    "# Make sure use cleaned version\n",
    "train = pd.read_csv('./emails.train.csv')\n",
    "test  = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Get labels\n",
    "Y_train = train['spam']\n",
    "Y_test  = test['spam']\n",
    "\n",
    "print(\"Extracting feature for train ...\")\n",
    "X_train = extract_Bag_of_Word_feature(train['text'])\n",
    "\n",
    "print(\"Extracting feature for test ...\")\n",
    "X_test  = extract_Bag_of_Word_feature(test[ 'text'])\n",
    "\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "def eval(model, X_test, Y_test, method=''):\n",
    "    print(\"====== Performance of: {method} =======\".format(method=method))\n",
    "    \n",
    "    # Predict decision labels.\n",
    "    Y_pred  = model.predict(X_test)  \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                              score=accuracy_score(Y_test, Y_pred)) )\n",
    "\n",
    "    # Predict confidence scores.\n",
    "    Y_score = model.decision_function(X_test)    \n",
    "    print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score)) )\n",
    "\n",
    "    # write to submit format\n",
    "    outf = 'kaggle_data/solution.%s.csv'% method\n",
    "    with open( outf, 'w') as f:\n",
    "        f.write('id,spam\\n')\n",
    "        for i in range(len(Y_pred)):\n",
    "            # print test['id'][0]\n",
    "            f.write('%s,%s\\n' % (test['id'][i], Y_pred[i]) )\n",
    "    print(\"[output] \"+outf)\n",
    "    \n",
    "    \n",
    "# evaluate current model\n",
    "eval(model, X_test, Y_test, method='SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the positive and negative examples in test is not balance (with Nr(pos)=1707, Nr(neg)=415 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Predict all labels as negative (=0)\n",
    "print(\"====== Predict as all negative =======\")\n",
    "Y_pred_all_neg  = np.zeros((len(Y_test),), dtype=np.int)\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Accuracy\", \n",
    "                                          score=accuracy_score(Y_test, Y_pred_all_neg)) )\n",
    "\n",
    "# Random guess performance\n",
    "print(\"====== Predict by random guess =======\")\n",
    "Y_score_rand = np.random.uniform(0,1, (len(Y_test),))    # Generate prediction score by random\n",
    "print(\"Metric[{metric:20s}]  {score:-3f}\".format( metric=\"Average Precision\", \n",
    "                                              score=average_precision_score(Y_test, Y_score_rand)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced: Data Preprocess\n",
    "\n",
    "However, obtaining good textural representation can be tricky as you may notice that the content of emails are noisy.\n",
    "We'll provide example code for text cleaning and you are expected to come up with smarter way to do it.\n",
    "\n",
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Om dit te runnen heb je de wordcloud module nodig.. als je op linux zit gewoon pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud(dataframe, title=None):\n",
    "    wordcloud = WordCloud(background_color=\"black\").generate(\" \".join([i for i in dataframe.str.upper()]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "# show the word cloud of orignial dataset.\n",
    "wordcloud(df['text'], 'orignial dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### **Stop words** \n",
    " Stop Words are words which do not contain important significance to be used in Search Queries. For example, 'a', 'the', 'is', 'as', etc. Usually these words need to be filtered out because they return vast amount of unnecessary information. \n",
    "\n",
    "- ### **Stemming**\n",
    " In linguistic morphology and information retrieval, stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\n",
    "\n",
    " Stemming programs are commonly referred to as stemming algorithms or stemmers.\n",
    "\n",
    "- ### **Lemmatization**\n",
    " Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "\n",
    " In computational linguistics, lemmatisation is the algorithmic process of determining the lemma for a given word. Since the process may involve complex tasks such as understanding context and determining the part of speech of a word in a sentence (requiring, for example, knowledge of the grammar of a language) it can be a hard task to implement a lemmatiser for a new language.\n",
    "\n",
    " In many languages, words appear in several inflected forms. For example, in English, the verb ‘to walk’ may appear as ‘walk’, ‘walked’, ‘walks’, ‘walking’. The base form, ‘walk’, that one might look up in a dictionary, is called the lemma for the word. The combination of the base form with the part of speech is often called the lexeme of the word.\n",
    "\n",
    " Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use python package *nltk* to do this. But before any operation, we need to download necessary nltk corpuses first with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download nltk corpus\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Instal Wordnet corpus                | Instal Stopwords corpus                |\n",
    "| ------------------------------------ |:--------------------------------------:|\n",
    "| ![alt text](images/nltk_wordnet.png) | ![alt text](images/nltk_stopwords.png) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_regularize(dataframe, method='lemm'):\n",
    "    print('Performing: %s ...' % method)\n",
    "    def stemming(worker, tag):\n",
    "        return worker.stem(tag)\n",
    "\n",
    "    def lemmatize(worker, tag):\n",
    "        return worker.lemmatize(tag)\n",
    "\n",
    "    if   method=='stem':\n",
    "        worker = nltk.PorterStemmer()\n",
    "        func = stemming\n",
    "    elif method=='lemm':\n",
    "        worker = nltk.WordNetLemmatizer()\n",
    "        func = lemmatize\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "#         haalt de woorden uit de dataframe\n",
    "        elems = line.strip().split()\n",
    "    \n",
    "        # apply stemming or lemmatize\n",
    "        newtags = [func(worker,tag.lower()) for tag in elems]\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i,'text'] = newline\n",
    "\n",
    "    # return dataframe\n",
    "\n",
    "def text_filtering(dataframe, extras=set()):\n",
    "    print('Performing: filtering ...')\n",
    "    import re\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    filter_set = set(stopwords.words('english'))\n",
    "    filter_set.update(extras)\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "        # remove special characters with regex\n",
    "        line = re.sub(r'[^\\w]', ' ', line)\n",
    "\n",
    "        # remove digits with regex\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "\n",
    "        # remove stop words\n",
    "        elems = line.strip().split()\n",
    "        newtags = filter(lambda x: x not in filter_set, elems)\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i, 'text'] = newline\n",
    "    # return dataframe\n",
    "    \n",
    "\n",
    "print('========= Clearn tranining data ==========')\n",
    "# Read in training data\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.train.csv')\n",
    "\n",
    "\n",
    "print('========= Clearn testing data ==========')\n",
    "# Read in testing data\n",
    "df = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Do cleaning\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "text_filtering(df, extras=set(['subject', 'ect', 'hou', '_']))\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.test.csv')\n",
    "\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now show the word cloud after cleaning.\n",
    "wordcloud(df['text'], 'After text cleaning')\n",
    "\n",
    "# Based on the observation from this word cloud, you may add more non-meaningful words into `extras'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "#HIER BEGINT DE ECHTE CODE\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "assert os.path.exists('./emails.train.csv'), \"[Dataset File Not Found] Please download dataset first.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get word frequency data for both train data\n",
    "def word_frequency_train(dataframe):\n",
    "    # for w in dataframe.str.lower():\n",
    "#     create a dataframe of all words\n",
    "    \n",
    "    datasetwords = {}\n",
    "    \n",
    "    for line in dataframe[\"text\"]:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in datasetwords:\n",
    "                datasetwords[word] += 1\n",
    "            else:\n",
    "                datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "# get word frequency data for mails\n",
    "def word_frequency_mail(mail):\n",
    "    words = mail.split()\n",
    "    datasetwords = {}\n",
    "    for word in words:\n",
    "        if word in datasetwords:\n",
    "            datasetwords[word] += 1\n",
    "        else:\n",
    "            datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# freqsword = frequency word in spam, freqhword is frequency word in ham\n",
    "def NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh):\n",
    "    \n",
    "#   probability of a mail being ham \n",
    "    probham = 1-probspam\n",
    "    \n",
    "#   calculate probability of word being in spam or word being in spam\n",
    "    probwordspam = float(freqsword)/(totalfreqs)\n",
    "    probwordham = float(freqhword)/(totalfreqh)\n",
    "    bayes = (float(probwordspam*probspam)/(probwordspam*probspam + probwordham*probham))\n",
    "\n",
    "\n",
    "#       equation for correcting the probability where s =3 means that meaning that the learned dictionary\n",
    "#       must contain more than 3 messages with the word\n",
    "    s = 3\n",
    "    correctedbayes = float(s*probspam+freqsword*bayes)/(s+freqsword)\n",
    "    \n",
    "#   if the spamicity of the word is around 0.5 return none since the probability of the word being ham or spam\n",
    "#   is too similar so we evaluate only spammicities of <=0.2 and >= 0.8\n",
    "    \n",
    "    if correctedbayes >= 0.8 or bayes <=0.2:\n",
    "        return correctedbayes\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here we call our Naive Bayesian performer with the data we read in from train and test and check\n",
    "# if the test mail is spam or not\n",
    "def CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "    spamchecklist = []\n",
    "    \n",
    "    for word in mail:\n",
    "    \n",
    "#     if word in spam and ham check its frequency and perform the Bayesian as many times as needed\n",
    "        if word in freqspam and word in freqham:\n",
    "            for i in range(mail[word]):\n",
    "                freqsword = freqspam[word]\n",
    "                freqhword = freqham[word]\n",
    "                spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "    #   if the word is in neither spam or ham we continue the word iteration\n",
    "        elif word not in freqspam and word not in freqham:\n",
    "            continue\n",
    "\n",
    "    #  we initialize freqsword as a tiny number here cause else the Bayesian would not be able to pick up that \n",
    "    #  the word is in ham if the word never appears in spam but does appear in ham\n",
    "        elif word in freqham and word not in freqspam:\n",
    "            for i in range(mail[word]):\n",
    "                freqsword = 10**(-8)\n",
    "                freqhword = freqham[word]\n",
    "                spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "            \n",
    "#   Clean up the spamchecklist\n",
    "    spamchecklist = np.array(spamchecklist)\n",
    "    spamchecklist = spamchecklist[spamchecklist != np.array(None)]\n",
    "    \n",
    "################################################################\n",
    "#    Again zou eigenlijk correct meoten zijn maar werkt niet (dit selecteert van de data alleen de 10 beste entries)\n",
    "#     try:\n",
    "#         spamchecklist = np.take(spamchecklist,(np.argpartition(abs(0.5-spamchecklist), -10)[-10:]))\n",
    "#     except:\n",
    "#         spamchecklist = spamchecklist\n",
    "#  #####################################################################   \n",
    "\n",
    "    \n",
    "#   Count all probabilities in spamchecklist and calculate the overal probability of the mail being spam or ham\n",
    "    multiplicationsum = np.prod(spamchecklist)\n",
    "    ones = np.ones(spamchecklist.shape)\n",
    "    pminusonesum = np.prod(ones-spamchecklist)\n",
    "    \n",
    "#   Make sure we won't have conflicts with machine precision for if the mail is almost certainly spam we get very\n",
    "#   small numbers which add to 0 under machine precision\n",
    "    if pminusonesum + multiplicationsum ==0:\n",
    "        finaljudge = 1\n",
    "    else:   \n",
    "        finaljudge = multiplicationsum/(multiplicationsum+pminusonesum)\n",
    "        \n",
    "#########################################\n",
    "# DIT WERKT SOMEHOW BETER maar theoretisch onjuist dus niet super handig te gebruiken (voor de overal probability te berekenen)\n",
    "#     finaljudge = sum(spamchecklist/len(spamchecklist))\n",
    "###############################################\n",
    "    \n",
    "#   Return False if spam\n",
    "    if finaljudge > 0.5:\n",
    "        return True\n",
    "#   Return True if ham\n",
    "    if finaljudge < 0.5:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes as input frequency table of spam words, frequency table of ham words,the mail we have to check,\n",
    "# probability mail is spam, total frequency of words in spam and total frequency of words  in  ham\n",
    "def Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train):\n",
    "    iddict = {}\n",
    "    spamdict = {}\n",
    "    idlist = []\n",
    "    spamlist = [] \n",
    "    \n",
    "# loop through all the mails in the test and perform naive bayesian and add id number to the list\n",
    "    for i in test['id']:\n",
    "        idlist.append(i)\n",
    "        mail = test.loc[test['id'] == i, 'text'].iloc[0]        \n",
    "        mail = word_frequency_mail(mail)\n",
    "    \n",
    "#         if we get True from CallBae mail is spam and we add it to the spam dictionary\n",
    "        if CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "            spamlist.append(1)\n",
    "\n",
    "            freqspam = combine_dicts(freqspam,mail)\n",
    "            totalfreqs = sum(freqspam.values())      \n",
    "    \n",
    "#             If we get False from Callbae mail is added to ham dictionary and defined as ham\n",
    "        else:\n",
    "            spamlist.append(0)\n",
    "            freqham = combine_dicts(freqham,mail)\n",
    "            totalfreqh = sum(freqham.values())\n",
    "                   \n",
    "    \n",
    "#   create our submission with per id the indicator whether the mail is spam or not (1 or 0)\n",
    "    spamdict['spam'] = spamlist\n",
    "    iddict['id'] = idlist\n",
    "    spamdict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in spamdict.items()]))\n",
    "    iddict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in iddict.items()]))\n",
    "    submission = pd.concat([iddict,spamdict],axis = 1)\n",
    "    submission.set_index('id',inplace=True)\n",
    "    submission.to_csv('submission.csv')\n",
    "    return submission,idlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for combining dictionaries\n",
    "def combine_dicts(a, b, op=operator.add):\n",
    "    return dict(a.items() + b.items() +[(k, op(a[k], b[k])) for k in set(b) & set(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23842999414176919, 0.23081429408318688, 0.20855301698886936, 0.1622729935559461, 0.12478031634446397, 0.08670181605155243, 0.0421792618629174, 0.025190392501464556, 0.026947861745752782, 0.056824838898652606, 0.13884007029876977, 0.3380199179847686, 0.6906854130052724, 0.7486818980667839, 0.7568834212067955, 0.7568834212067955, 0.7574692442882249, 0.7574692442882249]\n"
     ]
    }
   ],
   "source": [
    "# intialize initials\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "hamdata = train[train.spam == 0]\n",
    "spamdata = train[train.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())\n",
    "\n",
    "\n",
    "# Call our  model\n",
    "submission,idlist = Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train)\n",
    "\n",
    "MCE = []\n",
    "# MEAN CONSEQUENTIAL EVALUATER:\n",
    "for j in np.arange(0.4,0.6,0.9):\n",
    "    probspam = j\n",
    "    submission,idlist = Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train)\n",
    "    ms = 0\n",
    "    \n",
    "    for i in idlist:\n",
    "        try:      \n",
    "            if test.loc[test['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "                ms +=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    MCE.append(1./submission.shape[0]*ms)\n",
    "\n",
    "\n",
    "\n",
    "print MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b33e96abd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b33e9563990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.1,1,0.05)\n",
    "plt.plot(x,MCE)\n",
    "plt.title(\"Naive Bayesian\")\n",
    "plt.ylabel(\"MCE (Mean consequential Error)\")\n",
    "plt.xlabel(\"Probability of mail being Spam\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d1 = dict(d.items()[len(d)/2:])\n",
    "# d2 = dict(d.items()[:len(d)/2])\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "n_objects = train.text.shape[0]\n",
    "train_test_split = 0.7\n",
    "train_size = int(n_objects * train_test_split)\n",
    "indices = np.arange(n_objects)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "train_set= train.iloc[train_indices]\n",
    "test_set= train.iloc[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamdata = train_set[train_set.spam == 0]\n",
    "spamdata = train_set[train_set.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0306545153273\n",
      "0.029826014913\n",
      "0.0289975144988\n",
      "0.029826014913\n",
      "0.029826014913\n",
      "0.0439105219553\n",
      "0.0439105219553\n",
      "0.029826014913\n",
      "0.029826014913\n",
      "0.0273405136703\n"
     ]
    }
   ],
   "source": [
    "# MEAN CONSEQUENTIAL EVALUATER:\n",
    "MCE = 0.4\n",
    "probspam = 0.5\n",
    "while MCE >0.028:\n",
    "    submission,idlist = Classifier(freqspam,freqham,test_set,probspam,totalfreqs,totalfreqh,train)\n",
    "    ms = 0\n",
    "    for i in range(20):\n",
    "        freqspam.pop( np.random.choice(freqspam.keys()) ) \n",
    "        freqham.pop( np.random.choice(freqham.keys()) ) \n",
    "\n",
    "    for i in idlist:\n",
    "        try:      \n",
    "            if test_set.loc[test_set['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "                ms +=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    MCE=1./submission.shape[0]*ms\n",
    "\n",
    "\n",
    "\n",
    "    print MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.022261277094317515, 0.022261277094317515]\n",
      "[0.022261277094317515, 0.022261277094317515, 0.022261277094317515]\n",
      "[0.022261277094317515, 0.022261277094317515, 0.022261277094317515, 0.020503807850029292]\n"
     ]
    }
   ],
   "source": [
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "hamdata = train[train.spam == 0]\n",
    "spamdata = train[train.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())\n",
    "probspam = 0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "submission,idlist = Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train)\n",
    "ms = 0\n",
    " \n",
    "MCE = []\n",
    "for i in idlist:\n",
    "    try:      \n",
    "        if test.loc[test['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "            ms +=1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "MCE.append(1./submission.shape[0]*ms)\n",
    "MCE.append(1./submission.shape[0]*ms)\n",
    "print MCE\n",
    "while np.array(MCE)[-2] - np.array(MCE)[-1] ==0 and np.array(MCE)[-2] - np.array(MCE)[-1]<0.01:\n",
    "    submission,idlist = Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train)\n",
    "    ms = 0\n",
    "    for i in range(20):\n",
    "        freqspam.pop( np.random.choice(freqspam.keys()) ) \n",
    "        freqham.pop( np.random.choice(freqham.keys()) ) \n",
    "\n",
    "    for i in idlist:\n",
    "        try:      \n",
    "            if test.loc[test['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "                ms +=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    MCE.append(1./submission.shape[0]*ms)\n",
    "    print MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 2,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
