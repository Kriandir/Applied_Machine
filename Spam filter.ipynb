{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "assert os.path.exists('./emails.train.csv'), \"[Dataset File Not Found] Please download dataset first.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def text_regularize(dataframe, method='lemm'):\n",
    "    print('Performing: %s ...' % method)\n",
    "    def stemming(worker, tag):\n",
    "        return worker.stem(tag)\n",
    "\n",
    "    def lemmatize(worker, tag):\n",
    "        return worker.lemmatize(tag)\n",
    "\n",
    "    if   method=='stem':\n",
    "        worker = nltk.PorterStemmer()\n",
    "        func = stemming\n",
    "    elif method=='lemm':\n",
    "        worker = nltk.WordNetLemmatizer()\n",
    "        func = lemmatize\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "#         haalt de woorden uit de dataframe\n",
    "        elems = line.strip().split()\n",
    "    \n",
    "        # apply stemming or lemmatize\n",
    "        newtags = [func(worker,tag.lower()) for tag in elems]\n",
    "        newline = \" \".join(newtags)\n",
    "\n",
    "        # update text\n",
    "        dataframe.loc[i,'text'] = newline\n",
    "\n",
    "    # return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Clean training data ==========\n",
      "Cleaning text...\n",
      "Performing: lemm ...\n",
      "Performing: stem ...\n",
      "========= Clean testing data ==========\n",
      "Cleaning text...\n",
      "Performing: lemm ...\n",
      "Performing: stem ...\n",
      "Cleaned the data\n"
     ]
    }
   ],
   "source": [
    "def cleaning_text(dataframe):\n",
    "    print('Cleaning text...')\n",
    "    import re\n",
    "    \n",
    "    for i, line in enumerate(dataframe['text']):\n",
    "        \n",
    "        # remove special characters using regex\n",
    "        line = re.sub(r'[^\\w]', ' ', line).lower()\n",
    "        # remove numeric characters using regex\n",
    "        line = re.sub(\"(^|\\W)\\d+($|\\W)\", \" \", line)\n",
    "           \n",
    "        # remove words of two characters or less\n",
    "        words = line.split()\n",
    "        resultwords = []\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if len(word) > 2:\n",
    "                resultwords.append(word)\n",
    "                \n",
    "        newline = ' '.join(resultwords)\n",
    "\n",
    "        dataframe.loc[i, 'text'] = newline\n",
    "\n",
    "    return dataframe\n",
    "        \n",
    "print('========= Clean training data ==========')\n",
    "# Read in training data\n",
    "df = pd.read_csv('./emails.train.csv')\n",
    "\n",
    "# Do cleaning\n",
    "cleaning_text(df)\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.train.csv')\n",
    "\n",
    "print('========= Clean testing data ==========')\n",
    "# Read in testing data\n",
    "df = pd.read_csv('./emails.test.csv')\n",
    "\n",
    "# Do cleaning\n",
    "df = cleaning_text(df)\n",
    "text_regularize(df, 'lemm')\n",
    "text_regularize(df, 'stem')\n",
    "\n",
    "# Save as new file\n",
    "df.to_csv('emails_clean.test.csv')\n",
    "\n",
    "print('Cleaned the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# get word frequency data for both train data\n",
    "def word_frequency_train(dataframe):\n",
    "    # for w in dataframe.str.lower():\n",
    "#     create a dataframe of all words\n",
    "    \n",
    "    datasetwords = {}\n",
    "    \n",
    "    for line in dataframe[\"text\"]:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in datasetwords:\n",
    "                datasetwords[word] += 1\n",
    "            else:\n",
    "                datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "# get word frequency data for mails\n",
    "def word_frequency_mail(mail):\n",
    "    words = mail.split()\n",
    "    datasetwords = {}\n",
    "    for word in words:\n",
    "        if word in datasetwords:\n",
    "            datasetwords[word] += 1\n",
    "        else:\n",
    "            datasetwords[word] = 1\n",
    "    \n",
    "    return datasetwords\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freqsword = frequency word in spam, freqhword is frequency word in ham\n",
    "def NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh):\n",
    "    \n",
    "#   probability of a mail being ham \n",
    "    probham = 1-probspam\n",
    "    \n",
    "#   calculate probability of word being in spam or word being in spam\n",
    "    probwordspam = float(freqsword)/(totalfreqs)\n",
    "    probwordham = float(freqhword)/(totalfreqh)\n",
    "    bayes = (float(probwordspam*probspam)/(probwordspam*probspam + probwordham*probham))\n",
    "\n",
    "\n",
    "#       equation for correcting the probability where s =3 means that meaning that the learned dictionary\n",
    "#       must contain more than 3 messages with the word\n",
    "    s = 3\n",
    "    correctedbayes = float(s*probspam+freqsword*bayes)/(s+freqsword)\n",
    "    \n",
    "#   if the spamicity of the word is around 0.5 return none since the probability of the word being ham or spam\n",
    "#   is too similar so we evaluate only spammicities of <=0.2 and >= 0.8\n",
    "    \n",
    "    if correctedbayes >= 0.8 or bayes <=0.2:\n",
    "        return correctedbayes\n",
    "\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here we call our Naive Bayesian performer with the data we read in from train and test and check\n",
    "# if the test mail is spam or not\n",
    "def CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "    spamchecklist = []\n",
    "    \n",
    "    for word in mail:\n",
    "    \n",
    "#     if word in spam and ham check its frequency and perform the Bayesian as many times as needed\n",
    "        if word in freqspam and word in freqham:\n",
    "            for i in range(mail[word]):\n",
    "                freqsword = freqspam[word]\n",
    "                freqhword = freqham[word]\n",
    "                spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "    #   if the word is in neither spam or ham we continue the word iteration\n",
    "        elif word not in freqspam and word not in freqham:\n",
    "            continue\n",
    "\n",
    "    #  we initialize freqsword as a tiny number here cause else the Bayesian would not be able to pick up that \n",
    "    #  the word is in ham if the word never appears in spam but does appear in ham\n",
    "        elif word in freqham and word not in freqspam:\n",
    "            for i in range(mail[word]):\n",
    "                freqsword = 10**(-8)\n",
    "                freqhword = freqham[word]\n",
    "                spamchecklist.append(NaiveBayesian(probspam,freqsword,freqhword,totalfreqs,totalfreqh))\n",
    "\n",
    "            \n",
    "#   Clean up the spamchecklist\n",
    "    spamchecklist = np.array(spamchecklist)\n",
    "    spamchecklist = spamchecklist[spamchecklist != np.array(None)]\n",
    "    \n",
    "################################################################\n",
    "#    Again zou eigenlijk correct meoten zijn maar werkt niet (dit selecteert van de data alleen de 10 beste entries)\n",
    "#     try:\n",
    "#         spamchecklist = np.take(spamchecklist,(np.argpartition(abs(0.5-spamchecklist), -10)[-10:]))\n",
    "#     except:\n",
    "#         spamchecklist = spamchecklist\n",
    "#  #####################################################################   \n",
    "\n",
    "    \n",
    "#   Count all probabilities in spamchecklist and calculate the overal probability of the mail being spam or ham\n",
    "    multiplicationsum = np.prod(spamchecklist)\n",
    "    ones = np.ones(spamchecklist.shape)\n",
    "    pminusonesum = np.prod(ones-spamchecklist)\n",
    "    \n",
    "#   Make sure we won't have conflicts with machine precision for if the mail is almost certainly spam we get very\n",
    "#   small numbers which add to 0 under machine precision\n",
    "    if pminusonesum + multiplicationsum ==0:\n",
    "        finaljudge = 1\n",
    "    else:   \n",
    "        finaljudge = multiplicationsum/(multiplicationsum+pminusonesum)\n",
    "        \n",
    "#########################################\n",
    "# DIT WERKT SOMEHOW BETER maar theoretisch onjuist dus niet super handig te gebruiken (voor de overal probability te berekenen)\n",
    "#     finaljudge = sum(spamchecklist/len(spamchecklist))\n",
    "###############################################\n",
    "    \n",
    "#   Return False if spam\n",
    "    if finaljudge > 0.5:\n",
    "        return True\n",
    "#   Return True if ham\n",
    "    if finaljudge < 0.5:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes as input frequency table of spam words, frequency table of ham words,the mail we have to check,\n",
    "# probability mail is spam, total frequency of words in spam and total frequency of words  in  ham\n",
    "def Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train):\n",
    "    iddict = {}\n",
    "    spamdict = {}\n",
    "    idlist = []\n",
    "    spamlist = [] \n",
    "    \n",
    "# loop through all the mails in the test and perform naive bayesian and add id number to the list\n",
    "    for i in range(len(test)):\n",
    "        idlist.append(test['id'][i])\n",
    "        mail = test['text'][i]        \n",
    "        mail = word_frequency_mail(mail)\n",
    "    \n",
    "#         if we get True from CallBae mail is spam and we add it to the spam dictionary\n",
    "        if CallBae(freqspam,freqham,mail,probspam,totalfreqs,totalfreqh):\n",
    "            spamlist.append(1)\n",
    "\n",
    "            freqspam = combine_dicts(freqspam,mail)\n",
    "            totalfreqs = sum(freqspam.values())      \n",
    "    \n",
    "#             If we get False from Callbae mail is added to ham dictionary and defined as ham\n",
    "        else:\n",
    "            spamlist.append(0)\n",
    "            freqham = combine_dicts(freqham,mail)\n",
    "            totalfreqh = sum(freqham.values())\n",
    "                   \n",
    "    \n",
    "#   create our submission with per id the indicator whether the mail is spam or not (1 or 0)\n",
    "    spamdict['spam'] = spamlist\n",
    "    iddict['id'] = idlist\n",
    "    spamdict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in spamdict.items()]))\n",
    "    iddict = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in iddict.items()]))\n",
    "    submission = pd.concat([iddict,spamdict],axis = 1)\n",
    "    submission.set_index('id',inplace=True)\n",
    "    submission.to_csv('submission.csv')\n",
    "    return submission,idlist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for combining dictionaries\n",
    "def combine_dicts(a, b, op=operator.add):\n",
    "    return dict(a.items() + b.items() +[(k, op(a[k], b[k])) for k in set(b) & set(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04569420035149385, 0.023432923257176334, 0.022261277094317515, 0.054481546572934976]\n"
     ]
    }
   ],
   "source": [
    "# intialize initials\n",
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "test  = pd.read_csv('./emails_clean.test.csv')\n",
    "hamdata = train[train.spam == 0]\n",
    "spamdata = train[train.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())\n",
    "\n",
    "\n",
    "MCE = []\n",
    "# MEAN CONSEQUENTIAL EVALUATER:\n",
    "for j in np.arange(0.4,0.6,0.05):\n",
    "    probspam = j\n",
    "    submission,idlist = Classifier(freqspam,freqham,test,probspam,totalfreqs,totalfreqh,train)\n",
    "    ms = 0\n",
    "    \n",
    "    for i in idlist:\n",
    "        try:      \n",
    "            if test.loc[test['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "                ms +=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    MCE.append(1./submission.shape[0]*ms)\n",
    "\n",
    "\n",
    "\n",
    "print MCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9543058   0.97656708  0.97773872  0.94551845]\n"
     ]
    }
   ],
   "source": [
    "MCE = np.array(MCE)\n",
    "print 1-MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./emails_clean.train.csv')\n",
    "n_objects = train.text.shape[0]\n",
    "train_test_split = 0.7\n",
    "train_size = int(n_objects * train_test_split)\n",
    "indices = np.arange(n_objects)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "train_set= train.iloc[train_indices]\n",
    "test_set= train.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hamdata = train_set[train_set.spam == 0]\n",
    "spamdata = train_set[train_set.spam == 1]\n",
    "freqham = word_frequency_train(hamdata)\n",
    "freqspam = word_frequency_train(spamdata)\n",
    "totalfreqs = sum(freqspam.values())\n",
    "totalfreqh = sum(freqham.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2744cd3ddb50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprobspam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msubmission\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreqspam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreqham\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprobspam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotalfreqs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotalfreqh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-aee310006716>\u001b[0m in \u001b[0;36mClassifier\u001b[1;34m(freqspam, freqham, test, probspam, totalfreqs, totalfreqh, train)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# loop through all the mails in the test and perform naive bayesian and add id number to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0midlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mmail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kriek/anaconda2/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kriek/anaconda2/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2475\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2476\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2477\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2478\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2479\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4404)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas/_libs/index.c:4087)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:14031)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13975)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# MEAN CONSEQUENTIAL EVALUATER:\n",
    "\n",
    "probspam = 0.5\n",
    "submission,idlist = Classifier(freqspam,freqham,test_set,probspam,totalfreqs,totalfreqh,train)\n",
    "ms = 0\n",
    "\n",
    "for i in idlist:\n",
    "    try:      \n",
    "        if test_set.loc[test_set['id'] == i, 'spam'].iloc[0] != submission[\"spam\"][i]:\n",
    "            ms +=1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "MCE=1./submission.shape[0]*ms\n",
    "\n",
    "\n",
    "\n",
    "print MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
